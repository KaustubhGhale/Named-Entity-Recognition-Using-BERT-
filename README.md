ABSTRACT
------------------------------------------------------------------------------------------------------------------------------------------------------------

Named Entity Recognition (NER) plays a critical role in numerous
natural language processing (NLP) applications, including information retrieval,
question answering, and conversational AI. Leveraging recent advances in deep
learning, specifically the Bidirectional Encoder Representations from Transformers (BERT), has shown promising results in enhancing NER capabilities.
In this paper, we present a BERT-based model fine-tuned on the CoNLL-2003
dataset, detailing our approach to dataset preparation, model training, and evaluation using the Transformers library and Weights and Biases (W&B) for tracking. Our findings demonstrate the model's ability to recognize complex entities
with high accuracy, making it a powerful tool for real-world applications.
